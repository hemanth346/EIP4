{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemanth346/EIP4/blob/master/Week2/Session2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P917iHL-RWFI",
        "colab_type": "text"
      },
      "source": [
        "## To Do\n",
        "Create a CNN architecture ```without using Fully Connected layers and Biases``` and build a model that classifies MNIST data with test accuracy of more than 99.4% in less than 20 epochs and with in 15K param\n",
        "\n",
        "#### Constraints:\n",
        "- No Fully Connected Layers\n",
        "- No Biases\n",
        "- Achieve 99.4% in any epoch\n",
        "- not more than 20 epochs\n",
        "- not more than 15K param\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtrpjcKeSsHE",
        "colab_type": "text"
      },
      "source": [
        "#### Summary : \n",
        "Approach is to start with very little parameters and add if and when necessary\n",
        "\n",
        "Model 1:\n",
        "\n",
        "Started with working on last assignment final network submitted. Using 8 kernels for first layer as that is GPU accelerated and created similar model to that of previous assignment submission\n",
        "- Not used batch norm or dropout or adaptive learning rate.. \n",
        "- Parameters : 4.5k \n",
        "- Validation accuracy : 77.3%\n",
        "\n",
        "Model 2:\n",
        "\n",
        "Added batch norm and very less dropout of 10%..While training added validation set to every epoch. \n",
        "\n",
        "- Dropout is stopped before last 2 conv layers so as not to lose on imp information for classification.\n",
        "- Parameters : 4.7k \n",
        "- Highest val accuracy : 99%\n",
        "\n",
        "Model 2 (with Learning rate scheduler):\n",
        "  Trained same model but with learning rate decay.\n",
        "  - Param : 4.7k\n",
        "  - Highest Val accuracy - 99.25%\n",
        "\n",
        "Model 2 (with 128 Batch size):\n",
        "Increased batch size to see if it can generalize faster.\n",
        " - model improved to give 99.31% highest val accuracy\n",
        "\n",
        "Training accuracy also has not crossed 99.5%, increasing paramters to achieve training accuracy\n",
        "\n",
        "Model 3:\n",
        "\n",
        "Changed first Conv block to 16,32 combination kernels to learn more patterns, combined these 32 to get 10 kernels and 2nd Conv block to 10, 20 kernels before classification\n",
        "\n",
        "- Param -  9,924\n",
        "- Max. Val accuracy - 99.38\n",
        " \n",
        "\n",
        "Model 4:\n",
        "\n",
        "Model 3 with 128 batch size\n",
        "\n",
        "- Param - 9,924\n",
        "- Max Val Accuracy - 99.34%\n",
        "\n",
        "*We can observe that learning training and val accuracy are almost similar. It seems model is not training anymore, seems like needs more training or more parameters*\n",
        "\n",
        "Adding more kernels should increase training accuracy and thereby validation accuracy\n",
        "\n",
        "Model 5:\n",
        "\n",
        "Same network as Model 3 changed 2nd Conv block similar to first with 10 channels as output\n",
        "\n",
        "- Param - 11,100\n",
        "- Max. Val Accuracy - 99.35%\n",
        "\n",
        "Adding new param didn't help very much.!\n",
        "\n",
        "Model 6: \n",
        "\n",
        "Reducing dropout percentage to 5% to increase training accuracy for model 3\n",
        "\n",
        "- *Param - 9,924*\n",
        "- *Max. Val Accuracy - 99.43%*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EomuYw3kgHs",
        "colab_type": "text"
      },
      "source": [
        "## Installs, Imports, Boiler Plate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiUsqWC7d3Oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi5W0AejrA2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='once')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUStmXM6eRv_",
        "colab_type": "code",
        "outputId": "fdc74dfe-f250-4abf-d531-299cd8eb9986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "import keras"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n",
            "Using TensorFlow backend.\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
            "  return f(*args, **kwds)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:5747: ResourceWarning: unclosed file <_io.TextIOWrapper name='/root/.keras/keras.json' mode='r' encoding='UTF-8'>\n",
            "  _config = json.load(open(_config_path))\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YV4e2qOeSQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atGj4aIeezbc",
        "colab_type": "code",
        "outputId": "a6cd99ab-492f-482a-bbbc-722fcce5d60e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Load pre-shuffled data into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egl3l6lufJQS",
        "colab_type": "code",
        "outputId": "8062b7f9-dba6-465e-8c00-54395cbc9941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc41f49d710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRB2IAnCDiTx/65XcTNOWsh5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WbsgGzIfk2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1) # adding channel\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPyErbQMgOGG",
        "colab_type": "code",
        "outputId": "fcbe2b91-360e-4c55-88a7-5eb62f9eaa08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A09kcIFdgbAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aka1w2ltg80j",
        "colab_type": "code",
        "outputId": "dee7cf2e-55bb-41fb-b541-25d79ef5cf7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# checking class lables\n",
        "y_train[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj1M2EfnhBjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting 1D class labels into 10D class matrices - One hot encoding\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1Sw4BWqhbuu",
        "colab_type": "code",
        "outputId": "255bd30d-f563-443a-9b70-f5636ae029a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl5Q4n7DhnCW",
        "colab_type": "text"
      },
      "source": [
        "### Function to visulaze 1st layer images after using filters on i/p images for MNIST\n",
        "\n",
        "**Usage** : `vis_img_in_filter(model, image:'numpy.ndarray', layer_name:'str'='conv2d_1')`\n",
        "\n",
        "> img is expected from train set and in keras inbuilt mnist data format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYIU4jukhql_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(model, image = X_train[2], \n",
        "                      layer_name = 'conv2d_1'):\n",
        "    img=np.array(image).reshape((1, 28, 28, 1)).astype(np.float64)\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "      \n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsnyV_-XKgJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIsW8U3FKf-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld1MjXcfyDO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKGL16cUNmFT",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIK4JmfAFdvb",
        "colab_type": "code",
        "outputId": "a1b74976-be05-4fce-ae91-b9075ee7dfd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Conv2D(8, 3, use_bias=False, activation='relu', input_shape=(28,28,1)))  #26x8\n",
        "model1.add(Conv2D(16, 3, use_bias=False, activation='relu')) #24x16\n",
        "\n",
        "model1.add(MaxPooling2D(2)) # 12x16\n",
        "model1.add(Conv2D(8, 1, use_bias=False, activation='relu')) #12X8\n",
        "\n",
        "model1.add(Conv2D(8, 3, use_bias=False, activation='relu')) #10x8\n",
        "model1.add(Conv2D(16, 3, use_bias=False, activation='relu')) # 8x16\n",
        "\n",
        "model1.add(MaxPooling2D(2)) #4x16\n",
        "\n",
        "model1.add(Conv2D(10, 3, use_bias=False, activation='relu')) # 2x10\n",
        "model1.add(MaxPooling2D(2)) #1x10\n",
        "\n",
        "model1.add(Flatten())\n",
        "model1.add(Activation('softmax'))\n",
        "model1.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 8)         72        \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 16)        1152      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 8)         128       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 8)         576       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 16)          1152      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 2, 2, 10)          1440      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 4,520\n",
            "Trainable params: 4,520\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHIYpN6fFdlP",
        "colab_type": "code",
        "outputId": "e0de642b-40d9-44bb-8fb7-a874a3343ccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "model1.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBdcqWTDFdgW",
        "colab_type": "code",
        "outputId": "3edfc7fa-ddc3-420b-b6af-93d8ba26a8a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model1.fit(X_train, y_train, batch_size=32, epochs=20, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 23s 377us/step - loss: 0.8139 - acc: 0.7089\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 14s 238us/step - loss: 0.6284 - acc: 0.7586\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 14s 233us/step - loss: 0.5878 - acc: 0.7671\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 14s 236us/step - loss: 0.5697 - acc: 0.7711\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 14s 235us/step - loss: 0.5584 - acc: 0.7730\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 14s 233us/step - loss: 0.5500 - acc: 0.7751\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 15s 246us/step - loss: 0.5457 - acc: 0.7766\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 14s 235us/step - loss: 0.5401 - acc: 0.7775\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 14s 233us/step - loss: 0.5358 - acc: 0.7782\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 14s 232us/step - loss: 0.5309 - acc: 0.7796\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 14s 236us/step - loss: 0.5290 - acc: 0.7793\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 14s 232us/step - loss: 0.5260 - acc: 0.7800\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 14s 233us/step - loss: 0.5241 - acc: 0.7804\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 14s 232us/step - loss: 0.5227 - acc: 0.7807\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 14s 232us/step - loss: 0.5203 - acc: 0.7814\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 14s 232us/step - loss: 0.5201 - acc: 0.7814\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 14s 232us/step - loss: 0.5169 - acc: 0.7823\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 14s 234us/step - loss: 0.5164 - acc: 0.7821\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 14s 240us/step - loss: 0.5155 - acc: 0.7825\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 14s 232us/step - loss: 0.5162 - acc: 0.7821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5053be1ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35NZ2j9hFdY0",
        "colab_type": "code",
        "outputId": "95bdd9fa-679d-4299-92ff-c9fef3f2a1a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "score = model1.evaluate(X_test, y_test)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 95us/step\n",
            "[0.5448738030910492, 0.7726]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5DiTR6jJ8_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Failing as we have less than 8 filters\n",
        "#  vis_img_in_filter(model1, image=X_train[2],layer_name = 'conv2d_1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO_fLtgQvhxE",
        "colab_type": "code",
        "outputId": "27135af5-50b9-4f34-a4b9-326a508d42c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Conv2D(8, 3, use_bias=False, activation='relu', input_shape=(28,28,1)))  #26x8\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.1))\n",
        "model2.add(Conv2D(16, 3, use_bias=False, activation='relu')) #24x16\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.1))\n",
        "\n",
        "model2.add(MaxPooling2D(2)) # 12x16\n",
        "model2.add(Conv2D(8, 1, use_bias=False, activation='relu')) #12X8\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.1))\n",
        "\n",
        "model2.add(Conv2D(8, 3, use_bias=False, activation='relu')) #10x8\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.1))\n",
        "model2.add(Conv2D(16, 3, use_bias=False, activation='relu')) # 8x16\n",
        "model2.add(BatchNormalization())\n",
        "\n",
        "model2.add(MaxPooling2D(2)) #4x16\n",
        "\n",
        "model2.add(Conv2D(10, 3, use_bias=False, activation='relu')) # 2x10\n",
        "model2.add(MaxPooling2D(2)) #1x10\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Activation('softmax'))\n",
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 26, 26, 8)         72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 24, 24, 16)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 12, 12, 8)         128       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 12, 12, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 10, 10, 8)         576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 10, 10, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 10, 10, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 8, 8, 16)          1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 2, 2, 10)          1440      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 4,744\n",
            "Trainable params: 4,632\n",
            "Non-trainable params: 112\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDgoHlkZxrwr",
        "colab_type": "code",
        "outputId": "22b83059-c3c5-4325-eddd-f30440ad4e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "model2.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model2.fit(X_train, y_train, batch_size=32, epochs=20, verbose=1, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 29s 478us/step - loss: 0.3969 - acc: 0.8763 - val_loss: 0.0888 - val_acc: 0.9723\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 28s 462us/step - loss: 0.1139 - acc: 0.9648 - val_loss: 0.0560 - val_acc: 0.9814\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 28s 459us/step - loss: 0.0860 - acc: 0.9731 - val_loss: 0.0474 - val_acc: 0.9849\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 28s 460us/step - loss: 0.0742 - acc: 0.9771 - val_loss: 0.0437 - val_acc: 0.9850\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 27s 456us/step - loss: 0.0673 - acc: 0.9785 - val_loss: 0.0413 - val_acc: 0.9864\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 28s 470us/step - loss: 0.0595 - acc: 0.9819 - val_loss: 0.0424 - val_acc: 0.9865\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 27s 454us/step - loss: 0.0557 - acc: 0.9825 - val_loss: 0.0404 - val_acc: 0.9877\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 28s 460us/step - loss: 0.0552 - acc: 0.9827 - val_loss: 0.0383 - val_acc: 0.9872\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 28s 461us/step - loss: 0.0519 - acc: 0.9838 - val_loss: 0.0346 - val_acc: 0.9894\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 27s 456us/step - loss: 0.0499 - acc: 0.9847 - val_loss: 0.0331 - val_acc: 0.9893\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 27s 456us/step - loss: 0.0467 - acc: 0.9848 - val_loss: 0.0322 - val_acc: 0.9904\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 27s 455us/step - loss: 0.0445 - acc: 0.9857 - val_loss: 0.0369 - val_acc: 0.9889\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 28s 461us/step - loss: 0.0439 - acc: 0.9862 - val_loss: 0.0319 - val_acc: 0.9900\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 28s 460us/step - loss: 0.0433 - acc: 0.9863 - val_loss: 0.0326 - val_acc: 0.9899\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 27s 457us/step - loss: 0.0423 - acc: 0.9860 - val_loss: 0.0335 - val_acc: 0.9899\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 27s 457us/step - loss: 0.0414 - acc: 0.9869 - val_loss: 0.0333 - val_acc: 0.9899\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 28s 461us/step - loss: 0.0398 - acc: 0.9871 - val_loss: 0.0354 - val_acc: 0.9892\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 28s 459us/step - loss: 0.0398 - acc: 0.9876 - val_loss: 0.0347 - val_acc: 0.9889\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 27s 456us/step - loss: 0.0377 - acc: 0.9878 - val_loss: 0.0312 - val_acc: 0.9895\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 27s 456us/step - loss: 0.0379 - acc: 0.9878 - val_loss: 0.0316 - val_acc: 0.9891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f504012fe80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dOBu6NlyA_m",
        "colab_type": "code",
        "outputId": "241755e3-d7a9-47cf-e887-186b275cb31f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "score = model2.evaluate(X_test, y_test)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 95us/step\n",
            "[0.031555487849854395, 0.9891]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcYz8OJ7zsQ1",
        "colab_type": "code",
        "outputId": "ae880cba-6394-426f-8405-6d9b2f9d8509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model2.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model2.fit(X_train, y_train, batch_size=32, epochs=20, verbose=1, validation_data=(X_test, y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 29s 483us/step - loss: 0.0585 - acc: 0.9816 - val_loss: 0.0433 - val_acc: 0.9859\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 28s 466us/step - loss: 0.0471 - acc: 0.9850 - val_loss: 0.0370 - val_acc: 0.9877\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 28s 459us/step - loss: 0.0422 - acc: 0.9865 - val_loss: 0.0278 - val_acc: 0.9903\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 28s 461us/step - loss: 0.0385 - acc: 0.9876 - val_loss: 0.0305 - val_acc: 0.9906\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 28s 461us/step - loss: 0.0352 - acc: 0.9886 - val_loss: 0.0283 - val_acc: 0.9905\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 28s 460us/step - loss: 0.0335 - acc: 0.9894 - val_loss: 0.0285 - val_acc: 0.9907\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 28s 471us/step - loss: 0.0334 - acc: 0.9892 - val_loss: 0.0269 - val_acc: 0.9916\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 28s 461us/step - loss: 0.0311 - acc: 0.9899 - val_loss: 0.0254 - val_acc: 0.9922\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 28s 462us/step - loss: 0.0292 - acc: 0.9905 - val_loss: 0.0275 - val_acc: 0.9921\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 28s 460us/step - loss: 0.0290 - acc: 0.9905 - val_loss: 0.0259 - val_acc: 0.9922\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 28s 460us/step - loss: 0.0283 - acc: 0.9908 - val_loss: 0.0268 - val_acc: 0.9910\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 28s 464us/step - loss: 0.0287 - acc: 0.9906 - val_loss: 0.0263 - val_acc: 0.9914\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 28s 465us/step - loss: 0.0268 - acc: 0.9913 - val_loss: 0.0254 - val_acc: 0.9921\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 28s 460us/step - loss: 0.0251 - acc: 0.9919 - val_loss: 0.0251 - val_acc: 0.9925\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 27s 454us/step - loss: 0.0256 - acc: 0.9917 - val_loss: 0.0256 - val_acc: 0.9912\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 28s 465us/step - loss: 0.0263 - acc: 0.9915 - val_loss: 0.0245 - val_acc: 0.9922\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 28s 461us/step - loss: 0.0253 - acc: 0.9914 - val_loss: 0.0258 - val_acc: 0.9921\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 28s 469us/step - loss: 0.0249 - acc: 0.9917 - val_loss: 0.0261 - val_acc: 0.9908\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 28s 465us/step - loss: 0.0251 - acc: 0.9917 - val_loss: 0.0243 - val_acc: 0.9925\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 28s 459us/step - loss: 0.0237 - acc: 0.9924 - val_loss: 0.0247 - val_acc: 0.9915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4ffa928630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l26W5IDoz3d5",
        "colab_type": "code",
        "outputId": "7ab958ea-67dd-4027-a402-cd858b52af5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "score = model2.evaluate(X_test, y_test)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 95us/step\n",
            "[0.024743619471526472, 0.9915]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJoTVeIn2wdK",
        "colab_type": "code",
        "outputId": "f6477085-754d-483a-a1b1-d88b897602aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model2.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model2.fit(X_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 11s 180us/step - loss: 0.0297 - acc: 0.9897 - val_loss: 0.0303 - val_acc: 0.9910\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 8s 141us/step - loss: 0.0297 - acc: 0.9903 - val_loss: 0.0289 - val_acc: 0.9909\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 8s 138us/step - loss: 0.0273 - acc: 0.9911 - val_loss: 0.0294 - val_acc: 0.9914\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 8s 140us/step - loss: 0.0254 - acc: 0.9911 - val_loss: 0.0275 - val_acc: 0.9913\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 8s 140us/step - loss: 0.0240 - acc: 0.9921 - val_loss: 0.0244 - val_acc: 0.9922\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0233 - acc: 0.9922 - val_loss: 0.0258 - val_acc: 0.9919\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0225 - acc: 0.9924 - val_loss: 0.0255 - val_acc: 0.9917\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 9s 147us/step - loss: 0.0219 - acc: 0.9930 - val_loss: 0.0238 - val_acc: 0.9929\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0208 - acc: 0.9930 - val_loss: 0.0243 - val_acc: 0.9924\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0205 - acc: 0.9931 - val_loss: 0.0238 - val_acc: 0.9931\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0213 - acc: 0.9929 - val_loss: 0.0234 - val_acc: 0.9921\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0209 - acc: 0.9933 - val_loss: 0.0238 - val_acc: 0.9924\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0204 - acc: 0.9930 - val_loss: 0.0245 - val_acc: 0.9926\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0202 - acc: 0.9930 - val_loss: 0.0247 - val_acc: 0.9920\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0202 - acc: 0.9932 - val_loss: 0.0245 - val_acc: 0.9926\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 8s 142us/step - loss: 0.0192 - acc: 0.9938 - val_loss: 0.0252 - val_acc: 0.9921\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0201 - acc: 0.9930 - val_loss: 0.0241 - val_acc: 0.9920\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 8s 140us/step - loss: 0.0193 - acc: 0.9935 - val_loss: 0.0241 - val_acc: 0.9918\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0194 - acc: 0.9936 - val_loss: 0.0242 - val_acc: 0.9920\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 8s 140us/step - loss: 0.0191 - acc: 0.9937 - val_loss: 0.0245 - val_acc: 0.9924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4ffa63d5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz06KKfnwSCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aklM-ihiwR-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XiAwbfywR5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG7zBPIOwR20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqlo_VcWwRxj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dff981cc-5ef8-4d0a-95f2-13d901744067"
      },
      "source": [
        "model3 = Sequential()\n",
        "model3.add(Conv2D(16, 3, use_bias=False, activation='relu', input_shape=(28,28,1)))  #26x16\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.1))\n",
        "model3.add(Conv2D(32, 3, use_bias=False, activation='relu')) #24x32\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.1))\n",
        "\n",
        "model3.add(MaxPooling2D(2)) # 12x32\n",
        "model3.add(Conv2D(10, 1, use_bias=False, activation='relu')) #12X10\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.1))\n",
        "\n",
        "model3.add(Conv2D(10, 3, use_bias=False, activation='relu')) #10x10\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.1))\n",
        "model3.add(Conv2D(20, 3, use_bias=False, activation='relu')) # 8x20\n",
        "model3.add(BatchNormalization())\n",
        "\n",
        "model3.add(MaxPooling2D(2)) #4x20\n",
        "\n",
        "model3.add(Conv2D(10, 3, use_bias=False, activation='relu')) # 2x10\n",
        "model3.add(MaxPooling2D(2)) #1x10\n",
        "\n",
        "model3.add(Flatten())\n",
        "model3.add(Activation('softmax'))\n",
        "model3.summary()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 32)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 10)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 10)        900       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 10, 10, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 10, 10, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 20)          1800      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 20)          80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 2, 2, 10)          1800      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 9,924\n",
            "Trainable params: 9,748\n",
            "Non-trainable params: 176\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWsgKCJE3Cne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf62ffd2-9119-4c96-afdc-212741763364"
      },
      "source": [
        "model3.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model3.fit(X_train, y_train, batch_size=32, epochs=20, verbose=1, validation_data=(X_test, y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 33s 548us/step - loss: 0.1835 - acc: 0.9426 - val_loss: 0.0715 - val_acc: 0.9795\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 28s 463us/step - loss: 0.0689 - acc: 0.9793 - val_loss: 0.0461 - val_acc: 0.9854\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 28s 472us/step - loss: 0.0553 - acc: 0.9830 - val_loss: 0.0402 - val_acc: 0.9878\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 28s 472us/step - loss: 0.0459 - acc: 0.9852 - val_loss: 0.0342 - val_acc: 0.9888\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 28s 469us/step - loss: 0.0392 - acc: 0.9883 - val_loss: 0.0341 - val_acc: 0.9892\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 28s 468us/step - loss: 0.0365 - acc: 0.9885 - val_loss: 0.0298 - val_acc: 0.9899\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 28s 466us/step - loss: 0.0339 - acc: 0.9891 - val_loss: 0.0269 - val_acc: 0.9916\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 28s 472us/step - loss: 0.0298 - acc: 0.9905 - val_loss: 0.0306 - val_acc: 0.9902\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 28s 470us/step - loss: 0.0278 - acc: 0.9910 - val_loss: 0.0272 - val_acc: 0.9915\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 28s 473us/step - loss: 0.0278 - acc: 0.9913 - val_loss: 0.0266 - val_acc: 0.9915\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 28s 474us/step - loss: 0.0261 - acc: 0.9914 - val_loss: 0.0263 - val_acc: 0.9918\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 29s 480us/step - loss: 0.0235 - acc: 0.9921 - val_loss: 0.0216 - val_acc: 0.9936\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 29s 477us/step - loss: 0.0228 - acc: 0.9924 - val_loss: 0.0284 - val_acc: 0.9911\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 29s 478us/step - loss: 0.0220 - acc: 0.9927 - val_loss: 0.0256 - val_acc: 0.9922\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 29s 480us/step - loss: 0.0226 - acc: 0.9927 - val_loss: 0.0246 - val_acc: 0.9918\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 29s 478us/step - loss: 0.0210 - acc: 0.9931 - val_loss: 0.0236 - val_acc: 0.9931\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 29s 479us/step - loss: 0.0203 - acc: 0.9933 - val_loss: 0.0230 - val_acc: 0.9922\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 28s 475us/step - loss: 0.0200 - acc: 0.9934 - val_loss: 0.0242 - val_acc: 0.9916\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 29s 484us/step - loss: 0.0195 - acc: 0.9935 - val_loss: 0.0209 - val_acc: 0.9938\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 29s 476us/step - loss: 0.0179 - acc: 0.9941 - val_loss: 0.0230 - val_acc: 0.9927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc41efba198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh5RURGoyuCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "outputId": "bffc50a1-9a1e-47b5-c009-6d94c7f8f3a9"
      },
      "source": [
        "model4 = Sequential()\n",
        "model4.add(Conv2D(16, 3, use_bias=False, activation='relu', input_shape=(28,28,1)))  #26x16\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.1))\n",
        "model4.add(Conv2D(32, 3, use_bias=False, activation='relu')) #24x32\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.1))\n",
        "\n",
        "model4.add(MaxPooling2D(2)) # 12x32\n",
        "model4.add(Conv2D(10, 1, use_bias=False, activation='relu')) #12X10\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.1))\n",
        "\n",
        "model4.add(Conv2D(10, 3, use_bias=False, activation='relu')) #10x10\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.1))\n",
        "model4.add(Conv2D(20, 3, use_bias=False, activation='relu')) # 8x20\n",
        "model4.add(BatchNormalization())\n",
        "\n",
        "model4.add(MaxPooling2D(2)) #4x20\n",
        "\n",
        "model4.add(Conv2D(10, 3, use_bias=False, activation='relu')) # 2x10\n",
        "model4.add(MaxPooling2D(2)) #1x10\n",
        "\n",
        "model4.add(Flatten())\n",
        "model4.add(Activation('softmax'))\n",
        "model4.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 26, 26, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 24, 24, 32)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 12, 12, 10)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 12, 12, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 10, 10, 10)        900       \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 10, 10, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 10, 10, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 8, 8, 20)          1800      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 8, 8, 20)          80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 2, 2, 10)          1800      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 9,924\n",
            "Trainable params: 9,748\n",
            "Non-trainable params: 176\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiNymdA4ytvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08903975-a2e7-46ae-cff0-278066ab3f27"
      },
      "source": [
        "model4.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model4.fit(X_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 11s 182us/step - loss: 0.2798 - acc: 0.9136 - val_loss: 0.0621 - val_acc: 0.9810\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0745 - acc: 0.9769 - val_loss: 0.0512 - val_acc: 0.9842\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0566 - acc: 0.9820 - val_loss: 0.0455 - val_acc: 0.9863\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0487 - acc: 0.9850 - val_loss: 0.0384 - val_acc: 0.9867\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.0416 - acc: 0.9871 - val_loss: 0.0322 - val_acc: 0.9899\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0374 - acc: 0.9880 - val_loss: 0.0265 - val_acc: 0.9921\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0364 - acc: 0.9879 - val_loss: 0.0243 - val_acc: 0.9923\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0326 - acc: 0.9894 - val_loss: 0.0219 - val_acc: 0.9930\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.0319 - acc: 0.9898 - val_loss: 0.0246 - val_acc: 0.9920\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0301 - acc: 0.9904 - val_loss: 0.0216 - val_acc: 0.9931\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 9s 153us/step - loss: 0.0282 - acc: 0.9910 - val_loss: 0.0226 - val_acc: 0.9934\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0271 - acc: 0.9915 - val_loss: 0.0256 - val_acc: 0.9918\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0247 - acc: 0.9920 - val_loss: 0.0221 - val_acc: 0.9928\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0237 - acc: 0.9922 - val_loss: 0.0242 - val_acc: 0.9923\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0229 - acc: 0.9932 - val_loss: 0.0227 - val_acc: 0.9927\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0227 - acc: 0.9924 - val_loss: 0.0208 - val_acc: 0.9933\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0212 - acc: 0.9932 - val_loss: 0.0217 - val_acc: 0.9925\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.0204 - acc: 0.9935 - val_loss: 0.0212 - val_acc: 0.9928\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0223 - acc: 0.9929 - val_loss: 0.0230 - val_acc: 0.9921\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0237 - val_acc: 0.9918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc3b8f32d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFJWvMi8ytp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e_rEc1mytm7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "outputId": "726687f3-0bc5-4bb6-af18-bbc7c46ddf73"
      },
      "source": [
        "model5 = Sequential()\n",
        "model5.add(Conv2D(16, 3, use_bias=False, activation='relu', input_shape=(28,28,1)))  #26x16\n",
        "model5.add(BatchNormalization())\n",
        "model5.add(Dropout(0.1))\n",
        "model5.add(Conv2D(32, 3, use_bias=False, activation='relu')) #24x32\n",
        "model5.add(BatchNormalization())\n",
        "model5.add(Dropout(0.1))\n",
        "\n",
        "model5.add(MaxPooling2D(2)) # 12x32\n",
        "model5.add(Conv2D(16, 1, use_bias=False, activation='relu')) #12X16\n",
        "\n",
        "model5.add(BatchNormalization())\n",
        "model5.add(Dropout(0.1))\n",
        "\n",
        "model5.add(Conv2D(10, 3, use_bias=False, activation='relu')) #10x16\n",
        "model5.add(BatchNormalization())\n",
        "model5.add(Dropout(0.1))\n",
        "model5.add(Conv2D(32, 3, use_bias=False, activation='relu')) # 8x32\n",
        "\n",
        "model5.add(MaxPooling2D(2)) #4x32\n",
        "model5.add(Conv2D(10, 1, use_bias=False, activation='relu')) # 4x10\n",
        "\n",
        "model5.add(Conv2D(10, 3, use_bias=False, activation='relu')) # 2x10\n",
        "model5.add(MaxPooling2D(2)) #1x10\n",
        "\n",
        "model5.add(Flatten())\n",
        "model5.add(Activation('softmax'))\n",
        "model5.summary()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 26, 26, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 24, 24, 32)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 12, 12, 16)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 12, 12, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 12, 12, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 10, 10, 10)        1440      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 10, 10, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 10, 10, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 8, 8, 32)          2880      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 4, 4, 10)          320       \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 2, 2, 10)          900       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 11,100\n",
            "Trainable params: 10,952\n",
            "Non-trainable params: 148\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcyDiBvaytew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5acd6b7f-51da-456b-9c72-6b5ede848717"
      },
      "source": [
        "model5.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model5.fit(X_train, y_train, batch_size=32, epochs=20, verbose=1, validation_data=(X_test, y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 29s 484us/step - loss: 0.0640 - acc: 0.9800 - val_loss: 0.0407 - val_acc: 0.9872\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 29s 478us/step - loss: 0.0405 - acc: 0.9869 - val_loss: 0.0462 - val_acc: 0.9866\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 29s 482us/step - loss: 0.0325 - acc: 0.9894 - val_loss: 0.0348 - val_acc: 0.9894\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 29s 482us/step - loss: 0.0300 - acc: 0.9908 - val_loss: 0.0299 - val_acc: 0.9909\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 29s 479us/step - loss: 0.0260 - acc: 0.9919 - val_loss: 0.0250 - val_acc: 0.9915\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 29s 479us/step - loss: 0.0227 - acc: 0.9924 - val_loss: 0.0303 - val_acc: 0.9909\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 29s 476us/step - loss: 0.0218 - acc: 0.9930 - val_loss: 0.0259 - val_acc: 0.9915\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 29s 481us/step - loss: 0.0209 - acc: 0.9930 - val_loss: 0.0338 - val_acc: 0.9890\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 29s 479us/step - loss: 0.0189 - acc: 0.9938 - val_loss: 0.0271 - val_acc: 0.9916\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 29s 479us/step - loss: 0.0176 - acc: 0.9941 - val_loss: 0.0260 - val_acc: 0.9925\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 29s 479us/step - loss: 0.0173 - acc: 0.9940 - val_loss: 0.0272 - val_acc: 0.9920\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 29s 488us/step - loss: 0.0158 - acc: 0.9949 - val_loss: 0.0253 - val_acc: 0.9924\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 29s 482us/step - loss: 0.0158 - acc: 0.9949 - val_loss: 0.0258 - val_acc: 0.9920\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 29s 483us/step - loss: 0.0139 - acc: 0.9952 - val_loss: 0.0293 - val_acc: 0.9912\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 29s 480us/step - loss: 0.0149 - acc: 0.9950 - val_loss: 0.0296 - val_acc: 0.9919\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 29s 480us/step - loss: 0.0142 - acc: 0.9953 - val_loss: 0.0256 - val_acc: 0.9932\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 29s 481us/step - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0297 - val_acc: 0.9924\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 29s 477us/step - loss: 0.0141 - acc: 0.9953 - val_loss: 0.0265 - val_acc: 0.9935\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 29s 478us/step - loss: 0.0128 - acc: 0.9959 - val_loss: 0.0278 - val_acc: 0.9929\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 29s 476us/step - loss: 0.0132 - acc: 0.9956 - val_loss: 0.0250 - val_acc: 0.9933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc3b9335208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN_PKo1lGSBV",
        "colab_type": "text"
      },
      "source": [
        "# Final Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8mjAM9n-pwG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "outputId": "101b8706-337d-4a8d-e20e-6808a173e799"
      },
      "source": [
        "model6 = Sequential()\n",
        "model6.add(Conv2D(16, 3, use_bias=False, activation='relu', input_shape=(28,28,1)))  #26x16\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(Dropout(0.05))\n",
        "model6.add(Conv2D(32, 3, use_bias=False, activation='relu')) #24x32\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(Dropout(0.05))\n",
        "\n",
        "model6.add(MaxPooling2D(2)) # 12x32\n",
        "model6.add(Conv2D(10, 1, use_bias=False, activation='relu')) #12X10\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(Dropout(0.05))\n",
        "\n",
        "model6.add(Conv2D(10, 3, use_bias=False, activation='relu')) #10x10\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(Dropout(0.05))\n",
        "model6.add(Conv2D(20, 3, use_bias=False, activation='relu')) # 8x20\n",
        "model6.add(BatchNormalization())\n",
        "\n",
        "model6.add(MaxPooling2D(2)) #4x20\n",
        "\n",
        "model6.add(Conv2D(10, 3, use_bias=False, activation='relu')) # 2x10\n",
        "model6.add(MaxPooling2D(2)) #1x10\n",
        "\n",
        "model6.add(Flatten())\n",
        "model6.add(Activation('softmax'))\n",
        "model6.summary()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_20 (Conv2D)           (None, 26, 26, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 26, 26, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 26, 26, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 24, 24, 32)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 12, 12, 10)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 12, 12, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 10, 10, 10)        900       \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 10, 10, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 10, 10, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 8, 8, 20)          1800      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 8, 8, 20)          80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 4, 4, 20)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 2, 2, 10)          1800      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 9,924\n",
            "Trainable params: 9,748\n",
            "Non-trainable params: 176\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IERpPULBGFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad2b0ec8-1a30-4155-9b72-7c9a77620d00"
      },
      "source": [
        "model6.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model6.fit(X_train, y_train, batch_size=32, epochs=20, verbose=1, validation_data=(X_test, y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 29s 483us/step - loss: 0.0770 - acc: 0.9776 - val_loss: 0.0471 - val_acc: 0.9856\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 29s 487us/step - loss: 0.0403 - acc: 0.9876 - val_loss: 0.0394 - val_acc: 0.9870\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 29s 485us/step - loss: 0.0344 - acc: 0.9889 - val_loss: 0.0368 - val_acc: 0.9892\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 29s 486us/step - loss: 0.0279 - acc: 0.9909 - val_loss: 0.0360 - val_acc: 0.9890\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 29s 486us/step - loss: 0.0243 - acc: 0.9920 - val_loss: 0.0260 - val_acc: 0.9923\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 29s 488us/step - loss: 0.0229 - acc: 0.9923 - val_loss: 0.0279 - val_acc: 0.9927\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 29s 488us/step - loss: 0.0197 - acc: 0.9934 - val_loss: 0.0252 - val_acc: 0.9929\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 29s 491us/step - loss: 0.0192 - acc: 0.9938 - val_loss: 0.0222 - val_acc: 0.9934\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 29s 485us/step - loss: 0.0172 - acc: 0.9945 - val_loss: 0.0241 - val_acc: 0.9937\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 29s 483us/step - loss: 0.0163 - acc: 0.9946 - val_loss: 0.0243 - val_acc: 0.9932\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 29s 479us/step - loss: 0.0155 - acc: 0.9944 - val_loss: 0.0255 - val_acc: 0.9930\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 29s 484us/step - loss: 0.0137 - acc: 0.9955 - val_loss: 0.0220 - val_acc: 0.9936\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 29s 485us/step - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0245 - val_acc: 0.9928\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 29s 489us/step - loss: 0.0123 - acc: 0.9960 - val_loss: 0.0231 - val_acc: 0.9940\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 30s 493us/step - loss: 0.0106 - acc: 0.9966 - val_loss: 0.0246 - val_acc: 0.9933\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 29s 491us/step - loss: 0.0108 - acc: 0.9964 - val_loss: 0.0245 - val_acc: 0.9937\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 30s 498us/step - loss: 0.0103 - acc: 0.9967 - val_loss: 0.0227 - val_acc: 0.9933\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 30s 494us/step - loss: 0.0100 - acc: 0.9967 - val_loss: 0.0239 - val_acc: 0.9937\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 29s 490us/step - loss: 0.0091 - acc: 0.9971 - val_loss: 0.0230 - val_acc: 0.9935\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 29s 489us/step - loss: 0.0095 - acc: 0.9966 - val_loss: 0.0219 - val_acc: 0.9943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc3b83cca20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50HsSAImBJ6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}