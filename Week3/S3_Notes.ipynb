{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S3_Notes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCDneWFLmx_T",
        "colab_type": "text"
      },
      "source": [
        "> Since we may have to run multiple models each with 50 **epochs** and I may have comments on multiple models. This will be in separate notebook with links to other notebooks updated\n",
        "\n",
        "\n",
        "## To do:\n",
        "\n",
        "#### 1. Build a model with below constraints and beats validation score of existing model within 50 epochs\n",
        "\n",
        "1. Uses only depthwise separable convolution (no Conv2D)\n",
        "1.  Uses BatchNormalization \n",
        "1.  Has less than 100,000 parameters\n",
        "1.  Uses proper dropout values (based on own discretion)\n",
        "1.  Mention the output size for each layer\n",
        "1.  Mention the receptive field for each layer\n",
        "1.  Runs for 50 epochs\n",
        "1.  Beats the validation score within 50 epochs (at any epoch run, doesn't need to be final one)\n",
        "\n",
        "\n",
        "#### 2. For every network created \n",
        "1. Comment the `expected receptive field` and `expected output size` for every layer added to the network *`before model summary`*\n",
        "1. Add notes on why a change is made/why you think it will work\n",
        "1.  No dense layers\n",
        "1.  No Regular Conv2D layer without depthwise\n",
        "\n",
        "\n",
        "## Learnings (from class and assignment):\n",
        "- Different type of convolutions - Coming out of localized zone\n",
        "- checkboard issue\n",
        "- Types of Regularizations\n",
        "  - Dropout\n",
        "  - Weight Decay\n",
        "  - Dataset augmentation\n",
        "- Batch Normalization (2 learnable and 2 non-learnable parameters)\n",
        "- Difference between GAP and Avg. Pooling (Only Output shape)\n",
        "- Obtain Receptive Field and Output size for a layer\n",
        "- Making sense Accuracy and Loss plots\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jnpLPWvmzdW",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        " \n",
        "- Dataset : CIFAR10\n",
        "- No. of classes : 10\n",
        "- No. train images : 50000\n",
        "- No. test images : 10000\n",
        "\n",
        "\n",
        "> All models layers will have expected outsize and expected Receptive Field next to it\n",
        "\n",
        "\n",
        "All models are run for 50 epochs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOz48IXVmypr",
        "colab_type": "text"
      },
      "source": [
        "### Baseline Model:\n",
        "\n",
        "- Ran given model architecture on the data \n",
        "\n",
        "- Augmentations (already provided)\n",
        "  - Default values (No augmentation)\n",
        "\n",
        ">Total params: 1,172,410\n",
        "- Time taken : 14329.73 seconds \n",
        "- Max val_acc : 0.8293\n",
        "- Accuracy on test data is: 18.31 (?)\n",
        "\n",
        "\n",
        "\n",
        "Observations : \n",
        "\n",
        "  - Training accuracy:\n",
        "    - Accuary is increased after every epoch which means that model is learning\n",
        "    - Reached around 89% after 50 epochs\n",
        "\n",
        "  - Validation Accuracy:\n",
        "    - Accuracy has not increased much after 15 epochs and plateaued, even when training accuracy has been increasing.\n",
        "    - It can be inferred the learning is not generalized - and model **is overfitting**\n",
        "      - Since difference b/w training acc. and val_acc is more than ~5% we can also using regularizers to keep both of them around same value, so that we can train the model more, if required \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AdcVPFa9Hl4J"
      },
      "source": [
        "### Model 1:\n",
        "\n",
        "- Changed all convolution layers to separable convolution layers\n",
        "  - added BN and Dropout after each of separable layers\n",
        "- Followed Basics from session 2 (Conv blocks followed by transition blocks) \n",
        "- Transition block after receptive field of 5\n",
        "- Removed Dense layers and added Global Average Pooling at the end\n",
        "- Not used biases\n",
        "- No Augmentations used\n",
        "- Added weight Decay\n",
        "\n",
        "> **Total params: 13,019**\n",
        "- Model took 1147.28 seconds to train\n",
        "- **Max val_acc: 0.7702**\n",
        "- Accuracy on test data is: 75.91\n",
        "- **Max training accuracy reached is 79.3%**\n",
        "\n",
        "\n",
        "Observations:\n",
        "  - Model achieved good generalization\n",
        "  - Model has to be improved to achieve good training accuracy, and thereby increase validation accuracy as well. Can try to \n",
        "    - use more data(image augmentations) or\n",
        "    - train for more epochs (Constraint)\n",
        "    - try changing the model architecture\n",
        "\n",
        "*Trying a different architecture* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20H30vmftpky",
        "colab_type": "text"
      },
      "source": [
        "### Model 2:\n",
        "\n",
        "- Changed all convolution layers to Depthwise followed by a pointwise convolution(replaced separable convolutions)\n",
        "- Increased no. of filters\n",
        "- \n",
        "\n",
        "> **Total params: 35,387**\n",
        "- Model took 1104.17 seconds to train\n",
        "- **Max val_acc: 0.8023**\n",
        "- Accuracy on test data is: 80.23\n",
        "- **Max training acc: 0.8519**\n",
        "\n",
        "Observations:\n",
        "- From plot looks like good generalization\n",
        "\n",
        "Will increase Learning rate and try again if the score increases(check reference for source)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GyHBM2dtpg6",
        "colab_type": "text"
      },
      "source": [
        "### Model 3:\n",
        "\n",
        "- Model 2 with increased Learning rate and decreased decay\n",
        "\n",
        "> **Total params: 35,387**\n",
        "- Model took 1966.81 seconds to train\n",
        "- **Max val_acc: 0.8297**\n",
        "- Accuracy on test data is: 81.54\n",
        "- **Max training acc: 0.8959**\n",
        "\n",
        "Observations:\n",
        "- Model can be improved very much. Based on the constraints and the no. of paramters this model seems to be doing good\n",
        "\n",
        "- After writing receptive field the output layer is looking at a field of 58 which might be hindering the model. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Other models\n",
        "I've also ran some models, I've added the notebooks, will summarize shortly below\n",
        "\n",
        "**Model 4 :**\n",
        "\n",
        "Increase Dropout for same network \n",
        "  - Decreased both training(by ~5%) and validation (by ~2%) accuracy\n",
        "\n",
        "**Model 5:**\n",
        "\n",
        "Total params: 73,275\n",
        "\n",
        "Changed architecture to get more kernels in initial layers \n",
        "  - Max acc: 0.8945 \n",
        "  - Max val_acc: 0.8095\n",
        "\n",
        "Increased LR and changed LR scheduler rate based on plots: \n",
        "  - Max acc: 0.9013 \n",
        "  - Max val_acc: 0.8154\n",
        "\n",
        "**Model 6:**\n",
        "\n",
        "Added Augmentation to Model 1.\n",
        "\n",
        "No significant changes to accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bheYSgm157gX",
        "colab_type": "text"
      },
      "source": [
        "## Asignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLbKGVTO57YK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "```\n",
        "Epoch 1/50\n",
        "\n",
        "Epoch 00001: LearningRateScheduler setting learning rate to 0.01.\n",
        "390/390 [==============================] - 43s 110ms/step - loss: 0.8858 - acc: 0.6919 - val_loss: 2.1933 - val_acc: 0.5349\n",
        "Epoch 2/50\n",
        "\n",
        "Epoch 00002: LearningRateScheduler setting learning rate to 0.0075815011.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.7248 - acc: 0.7479 - val_loss: 0.8913 - val_acc: 0.6997\n",
        "Epoch 3/50\n",
        "\n",
        "Epoch 00003: LearningRateScheduler setting learning rate to 0.0061050061.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.6555 - acc: 0.7725 - val_loss: 0.9393 - val_acc: 0.6800\n",
        "Epoch 4/50\n",
        "\n",
        "Epoch 00004: LearningRateScheduler setting learning rate to 0.005109862.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.6131 - acc: 0.7864 - val_loss: 0.8299 - val_acc: 0.7163\n",
        "Epoch 5/50\n",
        "\n",
        "Epoch 00005: LearningRateScheduler setting learning rate to 0.0043936731.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.5760 - acc: 0.7994 - val_loss: 0.7299 - val_acc: 0.7529\n",
        "Epoch 6/50\n",
        "\n",
        "Epoch 00006: LearningRateScheduler setting learning rate to 0.0038535645.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.5466 - acc: 0.8080 - val_loss: 0.6988 - val_acc: 0.7580\n",
        "Epoch 7/50\n",
        "\n",
        "Epoch 00007: LearningRateScheduler setting learning rate to 0.003431709.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.5219 - acc: 0.8193 - val_loss: 0.6450 - val_acc: 0.7774\n",
        "Epoch 8/50\n",
        "\n",
        "Epoch 00008: LearningRateScheduler setting learning rate to 0.0030931024.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.5057 - acc: 0.8240 - val_loss: 0.6574 - val_acc: 0.7777\n",
        "Epoch 9/50\n",
        "\n",
        "Epoch 00009: LearningRateScheduler setting learning rate to 0.0028153153.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.4822 - acc: 0.8302 - val_loss: 0.5851 - val_acc: 0.8023\n",
        "Epoch 10/50\n",
        "\n",
        "Epoch 00010: LearningRateScheduler setting learning rate to 0.0025833118.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.4675 - acc: 0.8356 - val_loss: 0.5905 - val_acc: 0.7951\n",
        "Epoch 11/50\n",
        "\n",
        "Epoch 00011: LearningRateScheduler setting learning rate to 0.0023866348.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.4524 - acc: 0.8417 - val_loss: 0.5871 - val_acc: 0.8015\n",
        "Epoch 12/50\n",
        "\n",
        "Epoch 00012: LearningRateScheduler setting learning rate to 0.0022177866.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.4413 - acc: 0.8446 - val_loss: 0.6052 - val_acc: 0.7988\n",
        "Epoch 13/50\n",
        "\n",
        "Epoch 00013: LearningRateScheduler setting learning rate to 0.002071251.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.4251 - acc: 0.8496 - val_loss: 0.6016 - val_acc: 0.7954\n",
        "Epoch 14/50\n",
        "\n",
        "Epoch 00014: LearningRateScheduler setting learning rate to 0.0019428793.\n",
        "390/390 [==============================] - 40s 102ms/step - loss: 0.4161 - acc: 0.8527 - val_loss: 0.5925 - val_acc: 0.7987\n",
        "Epoch 15/50\n",
        "\n",
        "Epoch 00015: LearningRateScheduler setting learning rate to 0.0018294914.\n",
        "390/390 [==============================] - 40s 102ms/step - loss: 0.4091 - acc: 0.8560 - val_loss: 0.5593 - val_acc: 0.8126\n",
        "Epoch 16/50\n",
        "\n",
        "Epoch 00016: LearningRateScheduler setting learning rate to 0.0017286085.\n",
        "390/390 [==============================] - 40s 102ms/step - loss: 0.4017 - acc: 0.8595 - val_loss: 0.5559 - val_acc: 0.8135\n",
        "Epoch 17/50\n",
        "\n",
        "Epoch 00017: LearningRateScheduler setting learning rate to 0.00163827.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.3920 - acc: 0.8626 - val_loss: 0.5537 - val_acc: 0.8131\n",
        "Epoch 18/50\n",
        "\n",
        "Epoch 00018: LearningRateScheduler setting learning rate to 0.0015569049.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.3886 - acc: 0.8640 - val_loss: 0.5638 - val_acc: 0.8101\n",
        "Epoch 19/50\n",
        "\n",
        "Epoch 00019: LearningRateScheduler setting learning rate to 0.0014832394.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3787 - acc: 0.8665 - val_loss: 0.5686 - val_acc: 0.8095\n",
        "Epoch 20/50\n",
        "\n",
        "Epoch 00020: LearningRateScheduler setting learning rate to 0.00141623.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3764 - acc: 0.8672 - val_loss: 0.5774 - val_acc: 0.8064\n",
        "Epoch 21/50\n",
        "\n",
        "Epoch 00021: LearningRateScheduler setting learning rate to 0.0013550136.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3718 - acc: 0.8688 - val_loss: 0.5666 - val_acc: 0.8126\n",
        "Epoch 22/50\n",
        "\n",
        "Epoch 00022: LearningRateScheduler setting learning rate to 0.00129887.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3605 - acc: 0.8727 - val_loss: 0.5411 - val_acc: 0.8188\n",
        "Epoch 23/50\n",
        "\n",
        "Epoch 00023: LearningRateScheduler setting learning rate to 0.0012471938.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3579 - acc: 0.8721 - val_loss: 0.5596 - val_acc: 0.8146\n",
        "Epoch 24/50\n",
        "\n",
        "Epoch 00024: LearningRateScheduler setting learning rate to 0.0011994722.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3544 - acc: 0.8734 - val_loss: 0.5601 - val_acc: 0.8147\n",
        "Epoch 25/50\n",
        "\n",
        "Epoch 00025: LearningRateScheduler setting learning rate to 0.001155268.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3503 - acc: 0.8750 - val_loss: 0.5515 - val_acc: 0.8199\n",
        "Epoch 26/50\n",
        "\n",
        "Epoch 00026: LearningRateScheduler setting learning rate to 0.0011142061.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.3446 - acc: 0.8770 - val_loss: 0.5508 - val_acc: 0.8177\n",
        "Epoch 27/50\n",
        "\n",
        "Epoch 00027: LearningRateScheduler setting learning rate to 0.001075963.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.3384 - acc: 0.8783 - val_loss: 0.5531 - val_acc: 0.8197\n",
        "Epoch 28/50\n",
        "\n",
        "Epoch 00028: LearningRateScheduler setting learning rate to 0.001040258.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3327 - acc: 0.8812 - val_loss: 0.5560 - val_acc: 0.8162\n",
        "Epoch 29/50\n",
        "\n",
        "Epoch 00029: LearningRateScheduler setting learning rate to 0.0010068466.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3331 - acc: 0.8811 - val_loss: 0.5452 - val_acc: 0.8196\n",
        "Epoch 30/50\n",
        "\n",
        "Epoch 00030: LearningRateScheduler setting learning rate to 0.0009755146.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3292 - acc: 0.8830 - val_loss: 0.5593 - val_acc: 0.8190\n",
        "Epoch 31/50\n",
        "\n",
        "Epoch 00031: LearningRateScheduler setting learning rate to 0.0009460738.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3227 - acc: 0.8842 - val_loss: 0.5690 - val_acc: 0.8102\n",
        "Epoch 32/50\n",
        "\n",
        "Epoch 00032: LearningRateScheduler setting learning rate to 0.000918358.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3237 - acc: 0.8845 - val_loss: 0.5965 - val_acc: 0.8100\n",
        "Epoch 33/50\n",
        "\n",
        "Epoch 00033: LearningRateScheduler setting learning rate to 0.0008922198.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3183 - acc: 0.8864 - val_loss: 0.5641 - val_acc: 0.8182\n",
        "Epoch 34/50\n",
        "\n",
        "Epoch 00034: LearningRateScheduler setting learning rate to 0.0008675284.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3199 - acc: 0.8867 - val_loss: 0.5563 - val_acc: 0.8186\n",
        "Epoch 35/50\n",
        "\n",
        "Epoch 00035: LearningRateScheduler setting learning rate to 0.0008441668.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3157 - acc: 0.8875 - val_loss: 0.5556 - val_acc: 0.8203\n",
        "Epoch 36/50\n",
        "\n",
        "Epoch 00036: LearningRateScheduler setting learning rate to 0.0008220304.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3145 - acc: 0.8882 - val_loss: 0.5594 - val_acc: 0.8175\n",
        "Epoch 37/50\n",
        "\n",
        "Epoch 00037: LearningRateScheduler setting learning rate to 0.0008010253.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.3084 - acc: 0.8906 - val_loss: 0.5695 - val_acc: 0.8154\n",
        "Epoch 38/50\n",
        "\n",
        "Epoch 00038: LearningRateScheduler setting learning rate to 0.0007810669.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.3086 - acc: 0.8899 - val_loss: 0.5581 - val_acc: 0.8206\n",
        "Epoch 39/50\n",
        "\n",
        "Epoch 00039: LearningRateScheduler setting learning rate to 0.000762079.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.3062 - acc: 0.8915 - val_loss: 0.5552 - val_acc: 0.8193\n",
        "Epoch 40/50\n",
        "\n",
        "Epoch 00040: LearningRateScheduler setting learning rate to 0.0007439923.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.3010 - acc: 0.8925 - val_loss: 0.5729 - val_acc: 0.8189\n",
        "Epoch 41/50\n",
        "\n",
        "Epoch 00041: LearningRateScheduler setting learning rate to 0.0007267442.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.3074 - acc: 0.8899 - val_loss: 0.5724 - val_acc: 0.8151\n",
        "Epoch 42/50\n",
        "\n",
        "Epoch 00042: LearningRateScheduler setting learning rate to 0.0007102777.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.2969 - acc: 0.8935 - val_loss: 0.5695 - val_acc: 0.8174\n",
        "Epoch 43/50\n",
        "\n",
        "Epoch 00043: LearningRateScheduler setting learning rate to 0.0006945409.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.2998 - acc: 0.8930 - val_loss: 0.5655 - val_acc: 0.8196\n",
        "Epoch 44/50\n",
        "\n",
        "Epoch 00044: LearningRateScheduler setting learning rate to 0.0006794863.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.3009 - acc: 0.8912 - val_loss: 0.5634 - val_acc: 0.8174\n",
        "Epoch 45/50\n",
        "\n",
        "Epoch 00045: LearningRateScheduler setting learning rate to 0.0006650705.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.2941 - acc: 0.8958 - val_loss: 0.5672 - val_acc: 0.8183\n",
        "Epoch 46/50\n",
        "\n",
        "Epoch 00046: LearningRateScheduler setting learning rate to 0.0006512537.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.2957 - acc: 0.8953 - val_loss: 0.5565 - val_acc: 0.8221\n",
        "Epoch 47/50\n",
        "\n",
        "Epoch 00047: LearningRateScheduler setting learning rate to 0.0006379992.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.2935 - acc: 0.8938 - val_loss: 0.5555 - val_acc: 0.8297\n",
        "Epoch 48/50\n",
        "\n",
        "Epoch 00048: LearningRateScheduler setting learning rate to 0.0006252736.\n",
        "390/390 [==============================] - 39s 101ms/step - loss: 0.2913 - acc: 0.8950 - val_loss: 0.5675 - val_acc: 0.8197\n",
        "Epoch 49/50\n",
        "\n",
        "Epoch 00049: LearningRateScheduler setting learning rate to 0.0006130456.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.2934 - acc: 0.8950 - val_loss: 0.5803 - val_acc: 0.8171\n",
        "Epoch 50/50\n",
        "\n",
        "Epoch 00050: LearningRateScheduler setting learning rate to 0.0006012868.\n",
        "390/390 [==============================] - 39s 100ms/step - loss: 0.2868 - acc: 0.8959 - val_loss: 0.5797 - val_acc: 0.8154\n",
        "Model took 1966.81 seconds to train\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfjf2hH_57Ti",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "model = Sequential() \n",
        "model.add(DepthwiseConv2D(3, depth_multiplier=1, use_bias=False, padding='same', input_shape=(32, 32, 3))) # 32x3x3x3 RF:3\n",
        "model.add(Convolution2D(32, 1, use_bias=False, padding='same')) # 32x3x3x32 RF:3\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(DepthwiseConv2D(3, use_bias=False)) # 30x3x3x32 RF:5\n",
        "model.add(Convolution2D(64, 1, use_bias=False)) # 30x3x3x64 RF:5\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(DepthwiseConv2D(3, use_bias=False)) # 28x3x3x3 RF:7\n",
        "model.add(Convolution2D(128, 1, use_bias=False))  # 28x3x3x128 RF:7\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(MaxPooling2D(2)) # 14x3x3x128 RF:8\n",
        "model.add(Convolution2D(32, 1, use_bias=False))  # 14x3x3x32 RF:12\n",
        "\n",
        "model.add(DepthwiseConv2D(3, use_bias=False)) # 12x3x3x3 RF:16\n",
        "model.add(Convolution2D(32, 1, use_bias=False)) # 12x3x3x32 RF:16\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(DepthwiseConv2D(3, use_bias=False)) # 10x3x3x3 RF:20\n",
        "model.add(Convolution2D(64, 1, use_bias=False)) # 10x3x3x64 RF:20\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(DepthwiseConv2D(3, use_bias=False)) # 8x3x3x3 RF:24\n",
        "model.add(Convolution2D(128, 1, use_bias=False))  # 8x3x3x128 RF:24\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(MaxPooling2D(2)) # 4x3x3x128 RF:26\n",
        "model.add(Convolution2D(32, 1, use_bias=False))  # 4x3x3x32 RF:34\n",
        "\n",
        "model.add(DepthwiseConv2D(3, use_bias=False)) # 2x3x3x3 RF: 42  \n",
        "model.add(Convolution2D(32, 1, use_bias=False)) # 2x3x3x32 RF:50\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Convolution2D(10, 1, use_bias=False)) # 2x3x3x10 RF: 58\n",
        "model.add(GlobalAveragePooling2D()) # 1X10\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdA7_UcetpPl",
        "colab_type": "text"
      },
      "source": [
        "References : \n",
        "\n",
        "- For Understanding Plots : \n",
        "https://medium.com/@jonathan_hui/improve-deep-learning-models-performance-network-tuning-part-6-29bf90df6d2d\n",
        "\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0445ZkRtmUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}